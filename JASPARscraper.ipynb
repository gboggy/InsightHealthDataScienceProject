{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/my_projects_env/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file //anaconda/envs/my_projects_env/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "# The JASPAR website is saved locally and scraped for all human transcription factors to \n",
    "# mytuples.\n",
    "\n",
    "f = open(\"../JASPAR_web.html\")\n",
    "alltext = f.read()\n",
    "f.close()\n",
    "mytuples = re.findall(\n",
    "    r'>\\s*(\\w+\\.\\w+)\\s*&nbsp;\\s*</td>\\s*\\n.*\\n'\n",
    "    '.*<!-.+->\\n'\n",
    "    '.*<td.+>\\s*(\\w+.*)\\s*&nbsp;\\s*</td>\\s*'\n",
    "    '\\n.*\\n.*<!-.+->\\s*\\n.*'\n",
    "    '<td.+Homo sapiens.+</td>',\n",
    "    alltext, re.MULTILINE)\n",
    "\n",
    "filenames = [thing[0] for thing in mytuples]\n",
    "\n",
    "# Part 1\n",
    "# This block generates files (scraped in Part 2) from the individual transcription factor pages\n",
    "\n",
    "your_url_list = [\"http://jaspar.genereg.net/cgi-bin/jaspar_db.pl?ID=\" + filename +\"&rm=present&collection=CORE\" for filename in filenames]\n",
    "                \n",
    "file_name_list = filenames\n",
    "\n",
    "for index, url in enumerate(your_url_list):\n",
    "    try:\n",
    "        page = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(page)\n",
    "        html = str(soup.find_all(\"html\"))\n",
    "        \n",
    "        # Save the html to local hard drive first.\n",
    "        file = open(file_name_list[index], \"w\")\n",
    "        file.writelines(html)\n",
    "        file.close()\n",
    "        \n",
    "        random_no = (random.random()-0.5) * 2 * 1\n",
    "        time.sleep(2+random_no)\n",
    "    except IndexError:\n",
    "        pass\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# This block creates a database (.txt file) of transcription factor binding sites by scraping\n",
    "# the pages copied locally in Part 1.  The database is scanned against promoter sequences to \n",
    "# generate a Position Specific Motif Matrix (PSMM).  The database is structured according to \n",
    "# the format needed by CLOVER (used to generate the list of TF binding sites in each promoter).\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "open(\"outfile.txt\", \"w\")\n",
    "scrapped_data = []\n",
    "for tup in mytuples:\n",
    "    file = tup[0]\n",
    "    genename = tup[1]\n",
    "    soup = BeautifulSoup(open(file),\"lxml\")\n",
    "    with open(\"outfile.txt\", \"a\") as myfile:\n",
    "        myfile.write(\">\"+file+\" \"+genename+\"\\n\")\n",
    "    mat = soup.find_all(\"pre\")\n",
    "    textmat = str(mat)\n",
    "    textmat2 = re.findall(r'\\[<pre>(.*)\\n(.*)\\n(.*)\\n(.*\\])', textmat, re.MULTILINE)[0]\n",
    "\n",
    "    list_of_lists = []\n",
    "    for i in list(range(4)):\n",
    "        textmax3 = re.findall(r'\\[(.*)\\]',textmat2[i])[0].split(' ')\n",
    "        itemlist=[]\n",
    "        for item in textmax3:\n",
    "            try:\n",
    "                itemlist.append(int(item))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        list_of_lists.append(itemlist)\n",
    "    finalmat = np.array(list_of_lists)\n",
    "    finmat = finalmat.T\n",
    "    df = pd.DataFrame(finmat)\n",
    "    with open('outfile.txt', 'a') as f:\n",
    "        df.to_csv(f, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
